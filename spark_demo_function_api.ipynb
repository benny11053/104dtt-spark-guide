{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "import datetime\n",
    "\n",
    "import time\n",
    "import calendar\n",
    "import time\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import col,input_file_name\n",
    "from pyspark.sql.types import IntegerType, DateType, LongType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute spark engine \n",
    "spark = (SparkSession\n",
    "              .builder\\\n",
    "              .master(\"local[*]\")\\\n",
    "              .appName(\"spark_performace\")\\\n",
    "              .config(\"spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem\")\\\n",
    "              .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"com.amazonaws.auth.EnvironmentVariableCredentialsProvider\")\\\n",
    "              .config(\"spark.debug.maxToStringFields\", 2000)\\\n",
    "              .config(\"spark.sql.broadcastTimeout\",  36000)\\\n",
    "              .config(\"spark.sql.shuffle.partitions\", 128)\\\n",
    "              .config(\"spark.default.parallelism\", 16)\\\n",
    "              .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from s3\n",
    "sample_url = \"s3a://aws-test-benny/assessment/target_user_sample/*.parquet\"\n",
    "basic_url = \"s3a://aws-test-benny/assessment/basic/*.parquet\"\n",
    "exp_url = \"s3a://aws-test-benny/assessment/exp_job/*.parquet\"\n",
    "cid_url = \"s3a://aws-test-benny/assessment/cid_mapping/*.parquet\"\n",
    "\n",
    "sample_df= spark \\\n",
    "    .read \\\n",
    "    .parquet(sample_url)\n",
    "\n",
    "basic_df= spark \\\n",
    "    .read \\\n",
    "    .parquet(basic_url)\n",
    "\n",
    "exp_df = spark \\\n",
    "    .read \\\n",
    "    .parquet(exp_url)\n",
    "\n",
    "cid_df = spark \\\n",
    "    .read \\\n",
    "    .parquet(cid_url)\n",
    "    \n",
    "\n",
    "#sample_df.printSchema()\n",
    "#basic_df.printSchema()\n",
    "#exp_df.printSchema()\n",
    "#cid_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from database\n",
    "url = \"your db host\"\n",
    "dbname = \"your db name \"\n",
    "driver = \"com.mysql.cj.jdbc.Driver\"\n",
    "user = \"your db user\"\n",
    "password = \"your db password\"\n",
    "\n",
    "sample_dbtable = \"target_user_sample\"\n",
    "basic_dbtable = \"(SELECT id_no,update_date FROM basic) AS basic\"\n",
    "exp_dbtable = \"(SELECT pkey, id_no, invoice, ind_cat_no, job_cat_no, start_date, end_date FROM exp_job) AS exp\"\n",
    "cid_dbtable = \"(SELECT invoice, cid FROM cid_mapping) AS cid\"\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "sample_df= spark \\\n",
    "    .read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", url) \\\n",
    "    .option(\"driver\", driver) \\\n",
    "    .option(\"dbtable\", sample_dbtable) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"numPartitions\",9) \\\n",
    "    .option(\"partitionColumn\",\"finish_date\") \\\n",
    "    .option(\"lowerBound\", \"2012-01-01 00:00:00\") \\\n",
    "    .option(\"upperBound\",\"2019-01-01 00:00:00\") \\\n",
    "    .option(\"timestampFormat\", \"yyyy-mm-dd hh:mm:ss\") \\\n",
    "    .load()\n",
    "\n",
    "basic_df= spark \\\n",
    "    .read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", url) \\\n",
    "    .option(\"driver\", driver) \\\n",
    "    .option(\"dbtable\", basic_dbtable)\\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"numPartitions\",9) \\\n",
    "    .option(\"fetchsize\",500) \\\n",
    "    .load()\n",
    "\n",
    "exp_df = spark \\\n",
    "    .read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", url) \\\n",
    "    .option(\"driver\", driver) \\\n",
    "    .option(\"dbtable\",exp_dbtable) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"numPartitions\", 8) \\\n",
    "    .option(\"partitionColumn\",\"start_date\") \\\n",
    "    .option(\"lowerBound\", \"1993-01-01 00:00:00\") \\\n",
    "    .option(\"upperBound\",\"2020-01-01 00:00:00\") \\\n",
    "    .option(\"timestampFormat\", \"yyyy-mm-dd hh:mm:ss\") \\\n",
    "    .load()\n",
    "\n",
    "cid_df = spark \\\n",
    "    .read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", url) \\\n",
    "    .option(\"driver\", driver) \\\n",
    "    .option(\"dbtable\", cid_dbtable) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"numPartitions\", 8) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp JOIN basic ON idno JOIN sample_df ON idno\n",
    "# job_cat_no>0 or ind_cat_no>0\n",
    "\n",
    "join_df = exp_df \\\n",
    "            .where((f.col(\"job_cat_no\")>0) | (f.col(\"ind_cat_no\")>0))\\\n",
    "            .join(basic_df,[\"id_no\"]) \\\n",
    "            .join(sample_df, exp_df.id_no == sample_df.idno,'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_transform_df\n",
    "# 取代 end_date\n",
    "#\n",
    "# groupby pkey 取代 difftime drop duplicates\n",
    "# difftime = abs(((end_date-start_date)/2+start_date)-finish_date)\n",
    "# sm=  (end_y-start_y)*12+(end_m-start_m)\n",
    "# sy={\n",
    "#      sm=0 , 2+log(0.5/3)/log(2);\n",
    "#      o.w. ,  2+log(sm/3)/log(2)\n",
    "#         }\n",
    "# 先把邏輯做出來\n",
    "replace_condition = (f.datediff(f.col(\"end_date\"),f.col(\"start_date\"))<0)\n",
    "middle_date_diff = f.datediff(f.col(\"end_date\"),f.col(\"start_date\"))/2\n",
    "update_date_diff = f.datediff(f.col(\"start_date\"),f.col(\"finish_date\"))\n",
    "sm = (f.col(\"end_y\") - f.col(\"start_y\")) * 12 + (f.col(\"end_m\") - f.col(\"start_m\"))\n",
    "sy_default = f.lit(2)+f.lit(f.log(f.lit(0.5) / 3) / f.log(f.lit(2)))\n",
    "sy_by_sm = f.lit(2)+f.lit(f.log(f.lit(f.col(\"sm\")) / 3) / f.log(f.lit(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply 到 dataframe\n",
    "tmp_df = join_df \\\n",
    "    .withColumn(\"end_date\", \\\n",
    "                f.when(replace_condition, join_df[\"update_date\"]) \\\n",
    "                 .otherwise(join_df[\"end_date\"]) \\\n",
    "                      )\\\n",
    "    .withColumn(\"end_y\", f.year(f.col(\"end_date\")))\\\n",
    "    .withColumn(\"end_m\", f.month(f.col(\"end_date\"))) \\\n",
    "    .withColumn(\"start_y\", f.year(f.col(\"start_date\")))\\\n",
    "    .withColumn(\"start_m\",f.month(f.col(\"start_date\"))) \\\n",
    "    .withColumn(\"sm\", f.when(sm>750, f.lit(750)).otherwise(sm)) \\\n",
    "    .withColumn(\"sy\", f.when(f.col(\"sm\")==0,sy_default).otherwise(sy_by_sm))\n",
    "\n",
    "first_transform_df = tmp_df \\\n",
    "    .withColumn(\"diff_time\", f.abs(middle_date_diff+update_date_diff)) \\\n",
    "    .orderBy([\"pkey\",\"diff_time\"], ascending = True) \\\n",
    "    .coalesce(1) \\\n",
    "    .dropDuplicates([\"pkey\"])\\\n",
    "    .drop(\"pkey\",\"finish_date\",\"start_date\",\"end_date\",\"update_date\",\"idno\",\"end_y\",\"end_m\",\"start_y\",\"start_m\",\"diff_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order by  cid select where cid[0]\n",
    "# JOIN cid ON invoice\n",
    "# fill cid = 0 when invoice == 0 or invoice is null\n",
    "\n",
    "cid_select_df = cid_df \\\n",
    "                .select(\"cid\",\"invoice\") \\\n",
    "                .orderBy([\"invoice\",\"cid\"], ascending = True) \\\n",
    "                .coalesce(1) \\\n",
    "                .dropDuplicates([\"invoice\"])\n",
    "\n",
    "results = first_transform_df \\\n",
    "          .join(cid_select_df,[\"invoice\"],\"left\").na.fill({'cid': 0}) \\\n",
    "          .withColumn(\"cid\", f.when((f.col(\"invoice\")==0) | (f.col(\"cid\")==0) , f.lit(0))\\\n",
    "                    .otherwise(f.col(\"cid\"))) \n",
    "\n",
    "# write output\n",
    "results.write.format(\"parquet\").mode(\"overwrite\").save(\"s3a://aws-test-benny/sharon_inside/result.parquet\")\n",
    "\n",
    "#execution time\n",
    "execution_time = datetime.datetime.now() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spend time: 0:05:07.085671\n"
     ]
    }
   ],
   "source": [
    "print(f\"Spend time: {execution_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2214509"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
