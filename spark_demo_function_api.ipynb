{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "import datetime\n",
    "\n",
    "import time\n",
    "import calendar\n",
    "import time\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import col,input_file_name\n",
    "from pyspark.sql.types import IntegerType, DateType, LongType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#execute spark engine \n",
    "spark = (SparkSession\n",
    "              .builder\\\n",
    "              .master(\"local[*]\")\\\n",
    "              .appName(\"spark_performace\")\\\n",
    "              .config(\"spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem\")\\\n",
    "              .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"com.amazonaws.auth.EnvironmentVariableCredentialsProvider\")\\\n",
    "              .config(\"spark.hadoop.fs.s3a.endpoint\", \"s3-ap-northeast-1.amazonaws.com\")\\\n",
    "              .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\\\n",
    "              .config(\"spark.sql.execution.arrow.pyspark.enabled\",\"true\")\\\n",
    "              .config(\"spark.sql.parquet.fs.optimized.committer.optimization-enabled\",\"true\")\\\n",
    "              .config(\"spark.debug.maxToStringFields\", 2000)\\\n",
    "              .config(\"spark.sql.broadcastTimeout\",  36000)\\\n",
    "              .config(\"spark.sql.shuffle.partitions\", 128)\\\n",
    "              .config(\"spark.default.parallelism\", 16)\\\n",
    "              .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from database\n",
    "sample_url = \"s3a://aws-test-benny/assessment/target_user_sample/*.parquet\"\n",
    "basic_url = \"s3a://aws-test-benny/assessment/basic/*.parquet\"\n",
    "exp_url = \"s3a://aws-test-benny/assessment/exp_job/*.parquet\"\n",
    "cid_url = \"s3a://aws-test-benny/assessment/cid_mapping/*.parquet\"\n",
    "\n",
    "sample_df= spark \\\n",
    "    .read \\\n",
    "    .parquet(sample_url)\n",
    "\n",
    "basic_df= spark \\\n",
    "    .read \\\n",
    "    .parquet(basic_url)\n",
    "\n",
    "exp_df = spark \\\n",
    "    .read \\\n",
    "    .parquet(exp_url)\n",
    "\n",
    "cid_df = spark \\\n",
    "    .read \\\n",
    "    .parquet(cid_url)\n",
    "    \n",
    "\n",
    "#sample_df.printSchema()\n",
    "\n",
    "#basic_df.printSchema()\n",
    "\n",
    "#exp_df.printSchema()\n",
    "\n",
    "#cid_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from database\n",
    "url = f\"\"\n",
    "dbname = \"\"\n",
    "driver = \"com.mysql.cj.jdbc.Driver\"\n",
    "user = \"\"\n",
    "password = \"\"\n",
    "\n",
    "sample_dbtable = \"target_user_sample\"\n",
    "basic_dbtable = \"(SELECT id_no,update_date FROM basic) AS basic\"\n",
    "exp_dbtable = \"(SELECT pkey, id_no, invoice, ind_cat_no, job_cat_no, start_date, end_date FROM exp_job) AS exp\"\n",
    "cid_dbtable = \"(SELECT invoice, cid FROM cid_mapping) AS cid\"\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "sample_df= spark \\\n",
    "    .read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", url) \\\n",
    "    .option(\"driver\", driver) \\\n",
    "    .option(\"dbtable\", sample_dbtable) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"numPartitions\",9) \\\n",
    "    .option(\"partitionColumn\",\"finish_date\") \\\n",
    "    .option(\"lowerBound\", \"2012-01-01 00:00:00\") \\\n",
    "    .option(\"upperBound\",\"2019-01-01 00:00:00\") \\\n",
    "    .option(\"timestampFormat\", \"yyyy-mm-dd hh:mm:ss\") \\\n",
    "    .load()\n",
    "\n",
    "basic_df= spark \\\n",
    "    .read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", url) \\\n",
    "    .option(\"driver\", driver) \\\n",
    "    .option(\"dbtable\", basic_dbtable)\\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"numPartitions\",9) \\\n",
    "    .option(\"fetchsize\",500) \\\n",
    "    .load()\n",
    "\n",
    "exp_df = spark \\\n",
    "    .read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", url) \\\n",
    "    .option(\"driver\", driver) \\\n",
    "    .option(\"dbtable\",exp_dbtable) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"numPartitions\", 8) \\\n",
    "    .option(\"partitionColumn\",\"start_date\") \\\n",
    "    .option(\"lowerBound\", \"1993-01-01 00:00:00\") \\\n",
    "    .option(\"upperBound\",\"2020-01-01 00:00:00\") \\\n",
    "    .option(\"timestampFormat\", \"yyyy-mm-dd hh:mm:ss\") \\\n",
    "    .load()\n",
    "\n",
    "cid_df = spark \\\n",
    "    .read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", url) \\\n",
    "    .option(\"driver\", driver) \\\n",
    "    .option(\"dbtable\", cid_dbtable) \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .option(\"numPartitions\", 8) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o114.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.sql.SQLException: GC overhead limit exceeded\n\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:129)\n\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:97)\n\tat com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:955)\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeQuery(ClientPreparedStatement.java:1005)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:304)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:299)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2836)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2835)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2835)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.sql.SQLException: GC overhead limit exceeded\n\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:129)\n\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:97)\n\tat com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:955)\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeQuery(ClientPreparedStatement.java:1005)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:304)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-994b9fb24422>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#execution time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mexecution_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mbasic_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m         \"\"\"\n\u001b[0;32m--> 523\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o114.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.sql.SQLException: GC overhead limit exceeded\n\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:129)\n\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:97)\n\tat com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:955)\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeQuery(ClientPreparedStatement.java:1005)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:304)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:299)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2836)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2835)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2835)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.sql.SQLException: GC overhead limit exceeded\n\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:129)\n\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:97)\n\tat com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:955)\n\tat com.mysql.cj.jdbc.ClientPreparedStatement.executeQuery(ClientPreparedStatement.java:1005)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD.compute(JDBCRDD.scala:304)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# sample_df.printSchema()\n",
    "# basic_df.printSchema()\n",
    "# exp_df.printSchema()\n",
    "# cid_df.printSchema()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#basic_df.count()\n",
    "\n",
    "#exp_df.count()\n",
    "\n",
    "#cid_df.count()\n",
    "\n",
    "#execution time\n",
    "execution_time = datetime.datetime.now() - start_time\n",
    "basic_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Spend time: {execution_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp JOIN basic ON idno JOIN sample_df ON idno\n",
    "# job_cat_no>0 or ind_cat_no>0\n",
    "\n",
    "join_df = exp_df \\\n",
    ".where((f.col(\"job_cat_no\")>0) | (f.col(\"ind_cat_no\")>0))\\\n",
    ".join(basic_df,[\"id_no\"]) \\\n",
    ".join(sample_df, exp_df.id_no == sample_df.idno,'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_transform_df\n",
    "# 取代 end_date\n",
    "# groupby pkey 取代 difftime drop duplicates\n",
    "# difftime = abs(((end_date-start_date)/2+start_date)-finish_date)\n",
    "# sm=  (end_y-start_y)*12+(end_m-start_m)\n",
    "# sy={\n",
    "#      sm=0 , 2+log(0.5/3)/log(2);\n",
    "#      o.w. ,  2+log(sm/3)/log(2)\n",
    "#         }\n",
    "replace_condition = (f.datediff(f.col(\"end_date\"),f.col(\"start_date\"))<0)\n",
    "middle_date_diff = f.datediff(f.col(\"end_date\"),f.col(\"start_date\"))/2\n",
    "update_date_diff = f.datediff(f.col(\"start_date\"),f.col(\"finish_date\"))\n",
    "sm = (f.col(\"end_y\") - f.col(\"start_y\")) * 12 + (f.col(\"end_m\") - f.col(\"start_m\"))\n",
    "sy_default = f.lit(2)+f.lit(f.log(f.lit(0.5) / 3) / f.log(f.lit(2)))\n",
    "sy_by_sm = f.lit(2)+f.lit(f.log(f.lit(f.col(\"sm\")) / 3) / f.log(f.lit(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = join_df \\\n",
    "    .withColumn(\"end_date\", \\\n",
    "                       f.when(replace_condition, join_df[\"update_date\"]) \\\n",
    "                       .otherwise(join_df[\"end_date\"]) \\\n",
    "                      )\\\n",
    "    .withColumn(\"end_y\", f.year(f.col(\"end_date\")))\\\n",
    "    .withColumn(\"end_m\", f.month(f.col(\"end_date\"))) \\\n",
    "    .withColumn(\"start_y\", f.year(f.col(\"start_date\")))\\\n",
    "    .withColumn(\"start_m\",f.month(f.col(\"start_date\"))) \\\n",
    "    .withColumn(\"sm\", \\\n",
    "                f.when(sm>750, f.lit(750))\\\n",
    "                 .otherwise(sm)\n",
    "               ) \\\n",
    "    .withColumn(\"sy\", f.when(f.col(\"sm\")==0,sy_default)\\\n",
    "                       .otherwise(sy_by_sm)\n",
    "               )\n",
    "first_transform_df = tmp_df \\\n",
    "    .withColumn(\"diff_time\", f.abs(middle_date_diff+update_date_diff)) \\\n",
    "    .orderBy([\"pkey\",\"diff_time\"], ascending = True) \\\n",
    "    .coalesce(1) \\\n",
    "    .dropDuplicates([\"pkey\"])\\\n",
    "    .drop(\"pkey\",\"finish_date\",\"start_date\",\"end_date\",\"update_date\",\"idno\",\"end_y\",\"end_m\",\"start_y\",\"start_m\",\"diff_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order by  cid select where cid[0]\n",
    "# JOIN cid ON invoice\n",
    "# fill cid = 0 when invoice == 0 or invoice is null\n",
    "\n",
    "cid_select_df = cid_df \\\n",
    ".select(\"cid\",\"invoice\") \\\n",
    ".orderBy([\"invoice\",\"cid\"], ascending = True) \\\n",
    ".coalesce(1) \\\n",
    ".dropDuplicates([\"invoice\"])\n",
    "\n",
    "results = first_transform_df \\\n",
    ".join(cid_select_df,[\"invoice\"],\"left\").na.fill({'cid': 0}) \\\n",
    ".withColumn(\"cid\", f.when((f.col(\"invoice\")==0) | (f.col(\"cid\")==0) , f.lit(0))\\\n",
    "                    .otherwise(f.col(\"cid\"))) \n",
    "\n",
    "# write output\n",
    "results.write.format(\"parquet\").mode(\"overwrite\").save(\"s3a://aws-test-benny/sharon_inside/result.parquet\")\n",
    "\n",
    "#execution time\n",
    "execution_time = datetime.datetime.now() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spend time: 0:05:07.085671\n"
     ]
    }
   ],
   "source": [
    "print(f\"Spend time: {execution_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2214509"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- invoice: string (nullable = true)\n",
      " |-- id_no: long (nullable = true)\n",
      " |-- ind_cat_no: long (nullable = true)\n",
      " |-- job_cat_no: long (nullable = true)\n",
      " |-- tgu_id: long (nullable = true)\n",
      " |-- finish_date: timestamp (nullable = true)\n",
      " |-- z_A01: double (nullable = true)\n",
      " |-- z_A02: double (nullable = true)\n",
      " |-- z_A03: double (nullable = true)\n",
      " |-- z_A04: double (nullable = true)\n",
      " |-- z_A05: double (nullable = true)\n",
      " |-- z_A06: double (nullable = true)\n",
      " |-- z_A07: double (nullable = true)\n",
      " |-- z_B01: double (nullable = true)\n",
      " |-- z_B02: double (nullable = true)\n",
      " |-- z_B03: double (nullable = true)\n",
      " |-- z_B04: double (nullable = true)\n",
      " |-- z_B05: double (nullable = true)\n",
      " |-- z_B06: double (nullable = true)\n",
      " |-- z_B07: double (nullable = true)\n",
      " |-- z_C01: double (nullable = true)\n",
      " |-- z_C02: double (nullable = true)\n",
      " |-- z_C03: double (nullable = true)\n",
      " |-- z_C04: double (nullable = true)\n",
      " |-- z_C05: double (nullable = true)\n",
      " |-- z_C06: double (nullable = true)\n",
      " |-- z_C07: double (nullable = true)\n",
      " |-- z_I01: double (nullable = true)\n",
      " |-- z_I02: double (nullable = true)\n",
      " |-- z_I03: double (nullable = true)\n",
      " |-- z_I04: double (nullable = true)\n",
      " |-- z_I05: double (nullable = true)\n",
      " |-- z_I06: double (nullable = true)\n",
      " |-- z_I07: double (nullable = true)\n",
      " |-- z_I08: double (nullable = true)\n",
      " |-- z_L01: double (nullable = true)\n",
      " |-- z_L02: double (nullable = true)\n",
      " |-- z_L03: double (nullable = true)\n",
      " |-- z_L04: double (nullable = true)\n",
      " |-- z_L05: double (nullable = true)\n",
      " |-- z_L06: double (nullable = true)\n",
      " |-- z_L07: double (nullable = true)\n",
      " |-- z_L08: double (nullable = true)\n",
      " |-- z_R01: double (nullable = true)\n",
      " |-- z_R02: double (nullable = true)\n",
      " |-- z_R03: double (nullable = true)\n",
      " |-- z_R04: double (nullable = true)\n",
      " |-- z_R05: double (nullable = true)\n",
      " |-- z_R06: double (nullable = true)\n",
      " |-- z_R07: double (nullable = true)\n",
      " |-- z_R08: double (nullable = true)\n",
      " |-- z_R09: double (nullable = true)\n",
      " |-- z_S01: double (nullable = true)\n",
      " |-- z_S02: double (nullable = true)\n",
      " |-- z_S03: double (nullable = true)\n",
      " |-- z_S04: double (nullable = true)\n",
      " |-- z_S05: double (nullable = true)\n",
      " |-- z_S06: double (nullable = true)\n",
      " |-- z_S07: double (nullable = true)\n",
      " |-- z_D01: double (nullable = true)\n",
      " |-- z_D02: double (nullable = true)\n",
      " |-- z_D03: double (nullable = true)\n",
      " |-- z_D04: double (nullable = true)\n",
      " |-- z_D05: double (nullable = true)\n",
      " |-- z_D06: double (nullable = true)\n",
      " |-- z_D07: double (nullable = true)\n",
      " |-- z_D08: double (nullable = true)\n",
      " |-- z_D09: double (nullable = true)\n",
      " |-- z_D10: double (nullable = true)\n",
      " |-- z_D11: double (nullable = true)\n",
      " |-- z_D12: double (nullable = true)\n",
      " |-- z_D13: double (nullable = true)\n",
      " |-- z_D14: double (nullable = true)\n",
      " |-- z_D15: double (nullable = true)\n",
      " |-- z_D16: double (nullable = true)\n",
      " |-- z_D17: double (nullable = true)\n",
      " |-- z_D18: double (nullable = true)\n",
      " |-- z_D19: double (nullable = true)\n",
      " |-- z_D20: double (nullable = true)\n",
      " |-- z_D21: double (nullable = true)\n",
      " |-- z_D22: double (nullable = true)\n",
      " |-- z_D23: double (nullable = true)\n",
      " |-- z_D24: double (nullable = true)\n",
      " |-- z_D25: double (nullable = true)\n",
      " |-- z_D26: double (nullable = true)\n",
      " |-- z_D27: double (nullable = true)\n",
      " |-- z_D28: double (nullable = true)\n",
      " |-- sm: integer (nullable = true)\n",
      " |-- sy: double (nullable = true)\n",
      " |-- cid: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
